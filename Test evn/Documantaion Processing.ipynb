{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# flow = True \n"
   ],
   "id": "860a0ffdcd894253"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:38:14.641740Z",
     "start_time": "2024-09-25T08:38:14.608106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing necessary modules\n",
    "# - pandas: used for DataFrame operations.\n",
    "# - dill: used for saving and loading Python objects, including pipelines, to and from files.\n",
    "from ScoringPy import Processing\n",
    "import pandas as pd\n",
    "import dill\n",
    "\n",
    "# Sample dataset containing information about individuals:\n",
    "# The 'Age' column has missing (None) values that need to be handled.\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],  # Names of the individuals\n",
    "    'Age': [10, None, 20, None],  # Age of individuals; some values are missing (None)\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']  # City of residence\n",
    "}\n",
    "\n",
    "# Converting the dictionary to a pandas DataFrame for easier manipulation of tabular data.\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initializing the processing pipeline using the Processing class from the imported module.\n",
    "# Setting 'flow=True' means that the output from each function (step) will be passed as input to the next function.\n",
    "# This allows transformations to be applied sequentially, one step after another, without needing manual intervention.\n",
    "# The flow won't run immediately on initialization, allowing flexibility to add steps first.\n",
    "pipeline = Processing(flow=True)\n",
    "\n",
    "# Function that defines step 1 of the pipeline\n",
    "# This step fills the missing 'Age' values with the mean of the 'Age' column.\n",
    "# Args:\n",
    "# - data (DataFrame): The input DataFrame where 'Age' column has missing values.\n",
    "# Returns:\n",
    "# - DataFrame: The modified DataFrame where 'Age' has been filled with mean values.\n",
    "def step_1(data):\n",
    "    data['Age'] = data['Age'].fillna(data['Age'].mean())  # Filling missing 'Age' values with the mean of the 'Age' column.\n",
    "    return data\n",
    "\n",
    "# Function that defines step 2 of the pipeline\n",
    "# This step multiplies the 'Age' column values by 2.\n",
    "# Args:\n",
    "# - data (DataFrame): The input DataFrame from step 1, with no missing 'Age' values.\n",
    "# Returns:\n",
    "# - DataFrame: The modified DataFrame where 'Age' values are multiplied by 2.\n",
    "def step_2(data):\n",
    "    data['Age'] = data['Age'] * 2  # Doubling the 'Age' values.\n",
    "    return data\n",
    "\n",
    "# Function that defines step 3 of the pipeline\n",
    "# This step divides the 'Age' column values by 5.\n",
    "# Args:\n",
    "# - data (DataFrame): The input DataFrame from step 2, with doubled 'Age' values.\n",
    "# Returns:\n",
    "# - DataFrame: The modified DataFrame where 'Age' values are divided by 5.\n",
    "def step_3(data):\n",
    "    data['Age'] = data['Age'] / 5  # Dividing the 'Age' values by 5.\n",
    "    return data\n",
    "\n",
    "# Adding step 1 to the processing pipeline.\n",
    "# The pipeline will now execute step_1 as the first transformation when run.\n",
    "pipeline.add_step(step_1)\n",
    "\n",
    "# Adding step 2 to the processing pipeline.\n",
    "# The pipeline will execute step_2 after step_1 when the pipeline is run.\n",
    "pipeline.add_step(step_2)\n",
    "\n",
    "# Adding step 3 to the processing pipeline.\n",
    "# The pipeline will execute step_3 after step_2 when the pipeline is run.\n",
    "pipeline.add_step(step_3)\n",
    "\n",
    "# Saving the configured pipeline (with the steps added) to a file.\n",
    "# This allows for reusability of the pipeline without reconfiguring it each time.\n",
    "# The file is saved using the 'dill' library, which can serialize complex Python objects like classes and functions.\n",
    "with open('Pipeline.pkl', 'wb') as file:\n",
    "    dill.dump(pipeline, file)\n",
    "\n",
    "# Running the pipeline on the initial data (the 'df' DataFrame created earlier).\n",
    "# The pipeline will execute each step in sequence, as per the flow mechanism:\n",
    "# 1. Fill missing 'Age' values with the mean.\n",
    "# 2. Multiply 'Age' by 2.\n",
    "# 3. Divide 'Age' by 5.\n",
    "df = pipeline.run(initial_data=df)\n",
    "\n",
    "# The final DataFrame 'df' will have the following transformations:\n",
    "# - Missing values in 'Age' are replaced by the mean of the 'Age' column.\n",
    "# - The 'Age' values are multiplied by 2, and then divided by 5.\n"
   ],
   "id": "b082f62d78bfbb87",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline data cleared to free up memory.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Loading the pipeline for Filling Data\n",
    "# This section loads a previously saved pipeline from a file using the 'dill' library.\n",
    "# The file 'Pipeline.pkl' contains the serialized version of the pipeline that was configured and saved earlier.\n",
    "\n",
    "with open('Pipeline.pkl', 'rb') as file:\n",
    "    pipeline = dill.load(file)  # Loading the previously saved pipeline from the file.\n",
    "\n",
    "# The 'pipeline' object is now restored with all the previously added steps (step_1, step_2, and step_3).\n",
    "# These steps will be executed in sequence when the pipeline is run with the new input data.\n",
    "\n",
    "# Executing the pipeline with the DataFrame 'df', which was produced as a result of the first pipeline execution.\n",
    "# The input DataFrame 'df' contains transformations done by the earlier pipeline execution.\n",
    "# Here, we pass it through the same pipeline to reapply the same transformations, or to continue processing based on current data.\n",
    "\n",
    "df = pipeline.run(initial_data=df)  # Running the pipeline on the current DataFrame.\n",
    "\n",
    "# Clearing the pipeline\n",
    "# The pipeline object is cleared, meaning all the added steps will be removed.\n",
    "# This can be useful if you want to reset the pipeline and add new steps for different transformations.\n",
    "pipeline.clear()\n",
    "\n",
    "# At this point, the 'pipeline' object is empty, and no steps are available for execution until new steps are added.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-25T08:38:16.418765Z",
     "start_time": "2024-09-25T08:38:16.405629Z"
    }
   },
   "id": "febb7e6f6b9c1fdf",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# flow = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bac7704ac8c3c2e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "# - pandas: for working with DataFrame operations and reading Excel files.\n",
    "# - dill: for saving and loading Python objects, including the pipeline, to and from files.\n",
    "from ScoringPy import Processing\n",
    "import pandas as pd\n",
    "import dill\n",
    "\n",
    "# The file path for the first Excel file.\n",
    "row_path = 'Data/step1.xlsx'\n",
    "\n",
    "# Initializing the processing pipeline without running the flow immediately.\n",
    "# Setting flow=False means that the steps in the pipeline will not automatically pass data between them.\n",
    "# You will control the flow manually and decide when to pass data to subsequent steps.\n",
    "pipeline = Processing(flow=False)\n",
    "\n",
    "# Step 1: Load data from an Excel file (provided as 'path').\n",
    "# Args:\n",
    "# - path (str): The file path to read from.\n",
    "# Returns:\n",
    "# - DataFrame: The data read from the Excel file.\n",
    "def step_1(path=None):\n",
    "    data = pd.read_excel(path)  # Reading the data from the specified Excel file.\n",
    "    return data\n",
    "\n",
    "# Step 2: Load a different dataset from another Excel file ('Data/step2.xlsx').\n",
    "# No arguments are needed here, as the file path is hardcoded.\n",
    "# Returns:\n",
    "# - DataFrame: The data read from 'step2.xlsx'.\n",
    "def step_2():\n",
    "    data = pd.read_excel('Data/step2.xlsx')  # Reading data from 'step2.xlsx'.\n",
    "    return data\n",
    "\n",
    "# Step 3: Concatenate the results from step 1 and step 2.\n",
    "# It retrieves the data from the context (stored results from previous steps).\n",
    "# The context acts as a state where data from each step can be accessed by subsequent steps.\n",
    "# Returns:\n",
    "# - DataFrame: The concatenated DataFrame where 'Age' values are multiplied by 2.\n",
    "def step_3():\n",
    "    step_1_data = pipeline.context.get('step_1')  # Retrieving the result of step 1 from the pipeline context.\n",
    "    step_2_data = pipeline.context.get('step_2')  # Retrieving the result of step 2 from the pipeline context.\n",
    "\n",
    "    # Concatenating the two DataFrames from step 1 and step 2.\n",
    "    data = pd.concat([step_1_data, step_2_data], ignore_index=True)\n",
    "\n",
    "    data['Age'] = data['Age'] * 2  # Doubling the 'Age' values.\n",
    "    return data\n",
    "\n",
    "# Step 4: Further transformation of the data by dividing the 'Age' column by 5.\n",
    "# Args:\n",
    "# - data (DataFrame): The input DataFrame from step 3.\n",
    "# Returns:\n",
    "# - DataFrame: The modified DataFrame where 'Age' values are divided by 5.\n",
    "def step_4(data):\n",
    "    data['Age'] = data['Age'] / 5  # Dividing the 'Age' values by 5.\n",
    "    return data\n",
    "\n",
    "\n",
    "# Adding the steps to the pipeline\n",
    "# Each step is added sequentially, and the corresponding function is passed as an argument.\n",
    "# If the function requires parameters (e.g., step 1), the parameters are provided when adding the step.\n",
    "\n",
    "# Step 1: Reading data from the Excel file located at 'row_path'.\n",
    "pipeline.add_step(step_1, row_path)\n",
    "\n",
    "# Step 2: Reading data from 'Data/step2.xlsx' file.\n",
    "pipeline.add_step(step_2)\n",
    "\n",
    "# Step 3: Concatenating the data from steps 1 and 2.\n",
    "# flow=True ensures that the output from step 3 will automatically be passed to step 4.\n",
    "pipeline.add_step(step_3, flow=True)\n",
    "\n",
    "# Step 4: Applying further transformations to the data, with flow=True.\n",
    "# Here, flow=True ensures that the output from step 3 will be passed to step 4 automatically.\n",
    "pipeline.add_step(step_4, flow=True)\n",
    "\n",
    "# Saving the configured pipeline to a file for reuse in the future.\n",
    "# The 'dill' library is used to serialize the pipeline object.\n",
    "with open('Pipeline.pkl', 'wb') as file:\n",
    "    dill.dump(pipeline, file)\n",
    "\n",
    "# Running the pipeline\n",
    "# The pipeline is executed, and it flows through all the steps in sequence:\n",
    "# Step 1 (data loading), Step 2 (additional data loading), Step 3 (data concatenation and transformation),\n",
    "# and Step 4 (final transformation). The output from each step is automatically passed to the next step when flow=True.\n",
    "df = pipeline.run()\n",
    "\n",
    "# The resulting DataFrame 'df' will contain:\n",
    "# - Data loaded from two Excel files (concatenated).\n",
    "# - 'Age' values that were first multiplied by 2 in step 3 and then divided by 5 in step 4.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-25T08:38:19.849455Z",
     "start_time": "2024-09-25T08:38:19.800133Z"
    }
   },
   "id": "1dc7a8aaa55afd24",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline data cleared to free up memory.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Loading the pipeline for processing data.\n",
    "# The 'Pipeline.pkl' file contains the saved pipeline object, which was serialized using the 'dill' library.\n",
    "# This pipeline has multiple steps that were configured earlier for data transformation.\n",
    "# We load the pipeline to reuse it without needing to redefine or reconfigure it.\n",
    "\n",
    "with open('Pipeline.pkl', 'rb') as file:\n",
    "    pipeline = dill.load(file)  # Loading the previously saved pipeline from the file using 'dill'.\n",
    "\n",
    "# After loading, the pipeline is restored with all the previously added steps.\n",
    "# These steps can now be executed in sequence as they were originally configured.\n",
    "\n",
    "# Executing the pipeline.\n",
    "# The pipeline's 'run()' method is called to execute all the steps in the pipeline.\n",
    "# Since the pipeline was configured with 'flow=True' for certain steps, data will automatically be passed from one step to the next.\n",
    "# The steps will process the data according to the logic defined in each function (e.g., data loading, concatenation, transformation).\n",
    "df = pipeline.run()\n",
    "\n",
    "# At this point, the 'df' DataFrame will contain the final result after all transformations are applied by the pipeline.\n",
    "\n",
    "# Clearing the pipeline.\n",
    "# The 'clear()' method removes all the steps and clears the context of the pipeline.\n",
    "# This is useful if you want to reset the pipeline to a clean state before re-adding steps or reconfiguring it.\n",
    "pipeline.clear()\n",
    "\n",
    "# After clearing, the pipeline is now empty and ready for new steps if needed.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-25T08:38:20.823216Z",
     "start_time": "2024-09-25T08:38:20.787992Z"
    }
   },
   "id": "4094e1c1d2809ed8",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b86d56083a5e839f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
